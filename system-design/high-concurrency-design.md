# 高并发系统设计

**单一职责**原则规定每个类只有单一的功能，在这里可以引申为每一层拥有单一职 责，且层与层之间边界清晰；

**迪米特法则**原意是一个对象应当对其它对象有尽可能少的了解，在分层架构的体现是数据的交互不能跨层，只能在相邻层之间进行;

**开闭原则**要求软 件对扩展开放，对修改关闭。它的含义其实就是将抽象层和实现层分离，抽象层是对实现 层共有特征的归纳总结，不可以修改，但是具体的实现是可以无限扩展，随意替换的

***

### 提升系统性能

高可用,高性能,高并发.

可扩展性,需要针对平时流量和峰值流量

　　性能优化: 八二原则,20%的精力解决80%的性能问题.性能优化是需要数据支撑的,明确需要优化指标,同时也是一个持续的过程

平均值只能反映一段时间的性能,敏感度比较差，如果这段时间有少量慢请求时， 在平均值上并不能如实的反应 ,最大值存在过于敏感

分位值:如100个请求,对响应时间排序,如果排在90位的响应时间即90分位值.分位值越大，对于慢请求的影响就越敏感

设立性能优化的目标时通常会这样表述：在每秒 1 万次的请求量下，响应时间 99 分位值 在 10ms 以下.正常情况下200ms的响应时间用户是没有感知的.1s是能够感知到,但是可以接受,超过1s是不可接受的了.

**高并发下的性能优化**

一种是提高系统的处理核心数，另一种是减少单次任务的响应时间

**高可用系统设计的思路**

一个成熟系统的可用性需要从系统设计和系统运维两方面来做保障

　　系统设计需要做好:failover（故障转移）多活架构、超时控制以 及降级和限流 .降级是为了保证核心服务的稳定而牺牲非核心服务的做法.

比方说我们发一条微博会先经过 反垃圾服务检测，检测内容是否是广告，通过后才会完成诸如写数据库等逻辑 ,在面对高并发的场景下就需要关闭反垃圾检测这个相对耗时的操作,保证主体流程的稳定.

系统运维可以从灰度发布、故障演练两个方面来考虑如何提升系统的可用性

**高可扩展性的设计思路**

数据库、缓存、依赖的第三方、负载均衡、交换机带宽等等都是系统扩展时需要考虑的因素

存储扩展 拆分: 将复杂的问题简单化 .想庞大的系统拆分为多个子系统,并根据 业务 进行存储拆分,拆分为不同的数据库,同时还可以进行故障隔离,二次根据 数据 存储拆分即分库分表操作.

业务扩展 根据业务是否相同以及业务的核心接口与非核心接口进行拆分,分别放到核心池中与非核心池中.还可以根据客户端的类型进行拆分,如服务外网的定义为外网池,服务内部的定义为内部池.

***

### 数据库篇

**池化技术**

对于数据库连接池,一般在线上建议最小连接数控制在 10 左右，最大连 接数控制在 20～30 左右即可.

　　需要关注数据库连接池中的连接是否是有效连接,mysql中参数wait\_timeout是控制连接有效期的,如果连接闲置时间过长就会自动关闭,但是客户端使用方是无法感知的,再次使用这个连接就会报错. 启动一个线程来定期检测连接池中的连接是否可用，比如使用连接发送“select 1”的命 令给数据库看是否会抛出异常,则移除.C3P0 连接池可以这样检测,比较推荐. 或者在获取到连接之后，先校验连接是否可用.比如 DBCP 连 接池的 testOnBorrow 配置项,线上不建议开启会引入额外开销.

　　JDK自带的线程池在线程数大于coreThreadCount后,先就将任务放入队列,而不是直接创建线程执行,是由于默认是CPU密集型,CPU很忙处理不过来,再创建线程没有意义,相反只会增加线程上下文切换的开销,放入队列可以等待CPU空闲. 而Tomcat,在线程数大于coreThreadCount后,继续创建线程值maxThreadCount,web请求常常需要查询数据库是IO密集型.CPU比较空闲,可以立即处理更多的线程.

**数据库优化方案之主从读写分离**

原始的数据库我们称为主库，主要负责数据的写入，拷贝的目标 数据库称为从库，主要负责支持数据查询. 数据的拷贝，我们称为主从复制,MySQL 的主从复制是依赖于 binlog 的.读写分离可以解决突发的读流量

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/mysql-%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6.png" alt=""><figcaption></figcaption></figure>

可以使用一主多从的架构.并不是从库越多就可以抗越多的流量,每一个从库都需要主库创建一个log dump线程.主从复制也会有一些延迟.主从复制的延迟可以通过,设置完整的消息给下游读取(建议),或者放入完整的消息到缓存,也可直接读取主库.在发送完整的消息时,需要注意带宽. 尤其注意主从延迟的指标.可能会造成写入数据之后立刻读的时 候读取不到的情况

**数据库优化方案之访问数据库**

在客户端与数据库之间添加代理层,使用中间件统一管理访问,这类中间件内部管理着很多的数据源,常见的有美团的DBProxy.缺点是sql需要经历两次网络传输,有一定的性能损耗

**数据库优化方案之分库分表**

在 4 核 8G的云服务器上对 MySQL5.7 做 Benchmark，大概可以支撑 500TPS 和 10000QPS .

数据被分配到多个数据库节点上， 那么数据的写入请求也从请求单一主库变成了请求多个数据分片节点，在一定程度上也会提升并发写入的性能.常用16库64表.

　　水平拆分与垂直拆分. 水平拆分可以按照某一个字段的哈希值做拆分 ,将数据放到不同的库,或者按照某个时间字段,进行区间拆分.这种拆分方式会存在明显的热点,比较关注最近的数据.

问题1: 水平拆分根据区间段拆分或者Hash值拆分的字段称为 "分区键".假设使用了ID作为分区键,在面对昵称查询时,需要提前做好id与昵称的映射,如果再对昵称作为分区键费时费力,id与昵称做好映射只有两个字段,即使在应对其他字段时,再做一次映射比再做一次拆分划算.

问题2: 分库分表引入的另外一个问题是一些数据库的特性在实现时可能变得很困难.多库join无法实现,只可用代码join,或者对于count()操作,比方说将计数的数据单独存储在一张表中或者记录在 Redis 里面

**分库分表全局ID唯一性**

基于雪花算法搭建分布式发号器

　　全局ID最好是具有单调性的,可以在排序时,少维护一个字段,另外一个原因是 ID 有序也会提升数据的写入性能.索引 数据在 B+ 树中是有序排列的， 如果ID是有序的,直接在最后追加即可,而若不是有序增加的,会存在在中间插入,移动对应位置的数据到后面.此外UUID不能作为 ID 的另一个原因是它不具备业务含义.

Twitter的Snowflake 的核心思想是将 64bit 的二进制数字分成若干部分，每一部分都存储有特定含义的数据，比如说时间戳、机器 ID、序列号等等，最终生成全局唯一的有序 ID。它的标准算法是这样的：

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt=""><figcaption></figcaption></figure>

47位的时间戳可以用69年了.如果你的系统部署在多个机房，那么 10 位的机器 ID 可以继续划分为 2～3 位的 IDC 标示 （可以支撑 4 个或者 8 个 IDC 机房）和 7～8 位的机器 ID（支持 128-256 台机器）；12 位的序列号代表着每个节点每毫秒最多可以生成 4096 的 ID

雪花算法的部署方式

　　一种是嵌入到业务代码里，也就是分布在业务服务器中 ,在服务器内部调用则不需要跨网络调用,性能更好,但是需要更多的机器位数来支持更多的业务服务器.如果机器重启为了保证每次都是唯一且相同的机器ID,需要借助Zookeeper等分布式一致性组件

另外一个部署方式是作为独立的服务部署，也就是发号器服务,发号器以主备方式部署，同时运行的只有一个发号器，那么机器 ID 可 以省略，这样可以留更多的位数给最后的自增信息位,甚至可以将机器ID写入到配置文件中,则不需要第三方组件维护机器ID的唯一性了.

问题1: 依赖 于系统的时间戳，一旦系统时间不准，就有可能生成重复的 ID .

问题2: 如果请求发号器的 QPS 不高，比如说发号器每毫秒只发一个 ID，就会造成生成 ID 的末位永远是 1(每次判断当前时刻是否有发过id,若没有发过则从1开始,发过了id则累加,记录毫秒,请求进来都是未发过,所以导致的末位永远是1)，那么在分库分表时如果使用 ID 作为分区键就会造成库表分配的不均匀 .

解决: 时间戳不记录毫秒而是记录秒，这样在一个时间区间里可以多发出几个号，避免出现分库分表时数据分配不均.生成的序列号的起始号可以做一下随机，这一秒是 21，下一秒是 30，这样就会尽量的 均衡了

基于数据库生成 ID 的方案。这些方法根植于公司的业务，同样能解决分布式环境下 ID 全局唯一性的问题

**NoSQL与关系型数据库**

　　关系型数据库需要字段约束,且写入磁盘多是随机IO,还需考虑页分裂(发生页分裂,不可避免的需要做数据移动)问题,即使做了写入内存,再做批量写入磁盘操作,仍存在随机IO可能.

基于 LSM 树的存储引擎 解决随机IO问题.LSM 树（Log-Structured Merge Tree）牺牲了一定的读性能来换取写入数据的高性能， Hbase、Cassandra、LevelDB 都是用这种算法作为存储的引擎 数据首先会写入到一个叫做 MemTable 的内存结构中 ,有 Write Ahead Log 机制备份.累计到一定规模后,刷新生成一个新的文件，我们把这个文件叫做 SSTable（Sorted String Table),当 SSTable 达到一定数量时，我们会将这些 SSTable 合并，减少文件的数量，因为 SSTable 都是有序的，所以合并的速度也很快 .类似ES合并segment.从 LSM 树里面读数据时,先找MemTable,再找SSTable,只是因为数据被拆分成多个 SSTable，所以读取的效率会低于 B+ 树索引

***

### 缓存篇

　　通过 MMU（Memory Management Unit） 实现虚拟地址到实际内存地址的映射,为了避免每次都需要重新计算,引入了TLB（Translation Lookaside Buffer）的组件来缓存 最近转换过的虚拟地址，和物理地址的映射 .

HTTP 协议也是有缓存机制的 ,第一次请求一个静态资源是,服务端返回的响应头中包含Etag字段,客户端再次请求时在请求头中将“If-None-Match” 的值设置为Etag字段的值,服务端拿到后判断与之前的静态图片相比是否有变化,若没有则返回304状态,减少网络传输,浏览器继续使用缓存的图片信息.

缓存与缓冲冲并不相同,缓冲区是buffer,缓存是cache.

常见的缓存主要就是静态缓存、分布式缓存和热点本地缓存 .

　　缓存比较适合于读多写少的业务场景，并且数据最好带有一定的热点属性,缓存会给整体系统带来复杂度，并且会有数据不一致的风险 ,通常使用内存作为存储介质，但是内存并不是无限的

**Cache Aside（旁路缓存）策略**

读策略: 从缓存中读取数据； 如果缓存命中，则直接返回数据； 如果缓存不命中，则从数据库中查询数据； 查询到数据后，将数据写入到缓存中，并且返回给用户

写策略: 更新数据库中的记录； 删除缓存记录

理论上旁路缓存是有问题的,但是实际写入缓存的速度快于写入数据库的速度,所有缓存不一致的问题不存在.

　　Cache Aside 存在的最大的问题是当写入比较频繁时，缓存中的数据会被频繁地清理，这 样会对缓存的命中率有一些影响.一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁,性能会受影响;另一种做法同样也是在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样 即使出现缓存不一致的情况,缓存会很快的过期,也可以接受.

**Read/Write Through（读穿 / 写穿）策略**

核心原则是用户只与缓存打交道，由缓存和数据库通信，写入或者读取数据

Write Through 的策略是这样的：先查询要写入的数据是否在缓存中，如果存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，如果缓存中数据不存 在，我们把这种情况叫做“Write Miss（写失效 ) 写失效又分两种策略: Write Allocate（按写分 配,写入缓存,由缓存组件写入数据库) 和 “No- write allocate（不按写分配,直接写入数据库）” 后者没有写入缓存流程,效率更高

Guava Cache 中的 Loading Cache 就有 Read Through 策略的影子

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/%E8%AF%BB%E7%A9%BF-%E5%86%99%E4%BC%A0%E7%AD%96%E7%95%A5.png" alt=""><figcaption></figcaption></figure>

**Write Back（写回）策略**

这个策略的核心思想是在写入数据时只写入缓存，并且把缓存块儿标记为“脏”的。而脏块 儿只有被再次使用时才会将其中的数据写入到后端存储中,写策略如下

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/%E5%86%99%E5%9B%9E%E7%AD%96%E7%95%A5.png" alt=""><figcaption></figcaption></figure>

写回-读策略如下,是计算机体系 结构中的设计，比如我们在向磁盘中写数据时采用的就是这种策略。 无论是操作系统层面的 Page Cache，还是日志的异步刷盘，亦或是消息队列中消息的异步写入磁盘，大多采用了 这种策略。因为这个策略在性能上的优势毋庸置疑，它避免了直接写磁盘造成的随机写问 题，毕竟写内存和写磁盘的随机 I/O 的延迟相差了几个数量级

![](https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/%E5%86%99%E5%9B%9E-%E8%AF%BB%E7%AD%96%E7%95%A5.png)

**分布式缓存高可用**

部署多个节点,互为备份,高可用方案分3部分:客户端方案(在客户端配置多个缓存的节点，通过缓存写入和读取算法策略来实现分布 式，从而提高缓存的可用性 )、中间代理层方案(在应用代码和缓存节点之间增加代理层，客户端所有的写入和读取的请 求都通过代理层，而代理层中会内置高可用策略，帮助提升缓存系统的高可用 )和服务端方案(Redis Sentinel 方案 )

分片算法常见的就是 Hash 分片算法和一致性 Hash 分片算法两种

Hash分片: 对缓存的 Key 做哈希计算，然后对总的缓存节点个数取余,但是会出现节点个数减少,再次计算缓存时,找不到缓存的情况,命中率下降.

**一致性 Hash 算法**可以很好地解决增加和删减节点时，命中率下降的问题

将整个 Hash 值空间组织成一个虚拟的圆环，然后将缓存节点的 IP 地址 或者主机名做 Hash 取值后，放置在这个圆环上 ,将需要缓存的数据计算出Hash值后,顺时针存储到节点上.如果在node1和node2之间新增node5,只会影响到key2,key2会顺时针漂移到node5上,不会影响到其他key,可以保 证命中率不会大幅下降 .

不过这种会存在极端情况,若ABC三个节点,挂掉其中一个,顺时针的下一个节点将会承担前一个节点的所有压力,所以引入了虚拟节点的概念,一个存储节点会有多个虚拟节点分布在hash环上,即使一个节点被摘除,不会将全部的流量分担到一个节点.

其次，就是一致性 Hash 算法的脏数据问题(更新缓存Cache A中的数据k: 3 成为k: 4,但是突然连接不上,更新到了Cache B中, 过后又连接上但是数据被漂移更新到Cache B上 k : 4,再次查询时只能查询到Cache A中的K: 3. 会导致此缓存变为脏缓存).在使用一致性 Hash 算法时一定要设置缓存的过期时间,当发生漂移时，之前 存储的脏数据可能已经过期，就可以减少存在脏数据的几率.推荐4-6个节点最佳.

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/%E4%B8%80%E8%87%B4%E6%80%A7hash.png" alt=""><figcaption></figcaption></figure>

**中间代理层**

业界也有很多中间代理层方案，比如 Facebook 的Mcrouter，Twitter 的 Twemproxy，豌豆荚的Codis。

所有缓存的读写请求都是经过代理层完成的。代理层是无状态 的，主要负责读写请求的路由功能

**服务端方案**

Redis Sentinel 模式来解决主从 Redis 部署时的高可用问题 在主节点挂了以后自动将从节点提升为主节点，保证整体集群的可用性

在 Sentinel 中会配置 Master 的地址， Sentinel 会时刻监控 Master 的状态，当发现 Master 在配置的时间间隔内无响应，就认为 Master 已经挂了，Sentinel 会从从节点中选取一个提升为主节点，并且把所有其他的从节 点作为新主的从节点

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/redis-sentinel.png" alt=""><figcaption></figcaption></figure>

**解决缓存穿透**

大量的请求,在缓存中未查询到,在数据库中也没有查询到,少量的流量在系统承受范围内,无影响.

两种解决方案：回种空值以及使用布隆过滤器

```

  Object nullValue = new Object();
        try {
            Object valueFromDB = getFromDB(uid); // 从数据库中查询数据
            if (valueFromDB == null) {
                cache.set(uid, nullValue, 10); // 如果从数据库中查询到空值，就把空值写入缓存，设置较短
            } else {
                cache.set(uid, valueFromDB, 1000);
            }
        } catch(Exception e) {
            cache.set(uid, nullValue, 10);
        }
```

如果有大量获取未注册用户信息的请求，缓存内 就会有有大量的空值缓存，也就会浪费缓存的存储空间，如果缓存空间被占满了，还会剔除 掉一些已经被缓存的用户信息反而会造成缓存命中率的下降 .评估一下缓存容量是否能够支撑

使用布隆过滤器,则提前将数据存入布隆过滤器中,用多个hash函数对输入的数据算出在数组中的位置,数组中对应位置的值设置为 1,其他位置为0 用布隆过滤器来抗一波流量.会存在一定的误判,且不支持删除元素.删除元素可能其他数据也会映射在这个位置,导致误删.

**CDN：静态资源加速**

静态资源访问的关键点是就近访问 .CDN 就是将静态的资源分发到，位于多个地理位置机房中的服务器上，因此它 能很好地解决数据就近访问的问题，也就加快了静态资源的访问速度

DNS(Domain Name System) 域名解析结果分为: A记录和CNAME记录. A记录是返回一个IP,CNAME是返回一个域名.

域名解析式分级的.需要做多次DNS查询.这里以[www.baidu.com](https://www.baidu.com)解析

1. 域名解析请求先会检查本机的 hosts 文件,是否有配置对应的IP
2. 请求 Local DNS 是否有域名解析结果的缓存，如果有就返回，标识是 从非权威 DNS 返回的结果
3. 开始 DNS 的迭代查询 先请求根 DNS，根 DNS 返回顶级 DNS（.com） 的地址；再请求.com 顶级 DNS，得到 baidu.com 的域名服务器地址；再从 baidu.com 的域名服务器中查询到 [www.baidu.com](https://www.baidu.com) 对应的 IP 地址，返回这个 IP 地址 的同时，标记这个结果是来自于权威 DNS 的结果，同时写入 Local DNS 的解析结果缓存

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/DNS%E8%A7%A3%E6%9E%90.png" alt=""><figcaption></figcaption></figure>

　　多次解析有一个性能问题,可以在APP启动时,做域名解析,并将解析结果放入LRU缓存中,如果从缓存中无法取到,再走DNS解析流程,同时为了应对域名解析结果变化的情况,缓存应设置有效期.定期的更新缓存中的数据.

**让用户请求到达CDN节点**

将要访问的域名,通过CNAME进行配置,最终解析出结果后,获取到CDN的IP

**找到最近的CDN节点**

GSLB（Global Server Load Balance，全局负载均衡）, 它的含义是对于部署在不同地域 的服务器之间做负载均衡

可以通过发送数据包测量 RTT 的方式 来决定返回哪一个节点.是否能够从 CDN 节点上获取到资源还取决于 CDN 的同步延时,无法从选定的 CDN 节点上获取到数据，我们就不得不从源站获取数据，而 用户网络到源站的网络可能会跨越多个主干网，这样不仅性能上有损耗，也会消耗源站的带 宽，带来更高的研发成本,需要关注 CDN 的命中率和源站 的带宽情况 .

**数据迁移**

级联同步的方案,配置一个新库,新库的从库,写数据时分别写入3个库,然后将流量切到新库.

**改造后的副本组预热缓存**

1. 在云上部署多组 mc 的副本组，自建机房在接收到写入请求时，会优先写入自建机房的缓存节点，异步写入云上部署的 mc 节点；
2. 在处理自建机房的读请求时，会指定一定的流量，比如 10%，优先走云上的缓存节点，这样虽然也会走专线穿透回自建机房的缓存节点，但是流量是可控的；
3. 当云上缓存节点的命中率达到 90% 以上时，就可以在云上部署应用服务器，让云上的应用服务器完全走云上的缓存节点就可以了。使用了这种方式，我们可以实现缓存数据的迁移，又可以尽量控制专线的带宽和请求的延迟

***

### 消息队列篇

异步处理、解耦合和削峰填谷,比如用户购买后,积分以及优惠券服务可以异步处理.

**消息投递**

保证消息不丢失,保证消息只被消费一次

消息丢失的三个场景: 场景1. 消息从生产者写入到消息队列的过程。 场景2. 消息在消息队列中的存储场景。场景3. 消息被消费者消费的过程。

场景1: 可通过生产端重传即可,但是会出现写入重复消息到消息队列

场景2: 拿 Kafka 举例 消息在 Kafka 中是存储在本地磁盘上的，而为了减少消息存储时对磁盘的 随机 I/O，我们一般会将消息先写入到操作系统的 Page Cache 中，然后再找合适的时机刷 新到磁盘上,可以配置到一定的间隔时间,或者累计到一定的数据量异步刷盘,但是可能会存在在刷盘前断电问题.

　　考虑以集群方式部署 Kafka 服 务，通过部署多个副本备份数据，保证消息尽量不丢失 . Kafka集群中Leader复杂写入和消费,多个Follwer复杂备份,Follower中特殊的集合叫ISR(in sync replica),可以理解为leader的副本,同时写leader和ISR的.Leader宕机,从ISR中选择一个当Leader,因为Leader复制消息是异步复制给Follower,可以将"acks=all",产者发送的每一条消息除了发给 Leader 外还会发给所有的 ISR，并且必须得到 Leader 和所有 ISR 的确认后(1,3,4,5)才被认为发送成功,"acks=all"生产消息的性能会受影响.

如果不能容忍丢消息,就集群部署,同时配置ISR.如果容忍丢一定的消息,则无需集群部署,或者配置一个follower即可,这样可以提高性能

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/kafka-ISR.png" alt=""><figcaption></figcaption></figure>

场景3: 在消费的过程中存在消息丢失的可能

消费消息分3步骤: 接收消息、处理消息、更新消费进度 (offset)

一定需要在消费消息完成后再提交offset,但是存在提交offset之前,宕机,这样会存在重复消费的情况,所以在消费消息时应当判断.

**生产者端幂等性**

　　保证消息不丢失存在性能损耗和消息重复问题, Kafka 0.11和和 Pulsar都支持生产者幂等性,即虽然生产者端可能会重复生产消息,但是,服务端最终存储的是只有一条消息,具体实现,每个消息分配一个唯一的id,服务端会存储<消息ID,最后一个消息ID> 如果当前消息的ID和最后一个消息ID相同,则会丢弃当前消息.

**消费端幂等性**

通用处理方案: 生产者分配一个全局ID,消费者每次消费时判断从数据库中查询这个ID是否被消费过,每次消费后都存入数据库. 这样依旧会有个问题,如果消费了,但是在最后写入数据库的时候服务宕机,依然会存在重复消费情况,此时就需要使用事务来保证,保证消息处理和写入数据库状态一致.

业务层面处理: 使用乐观锁的机制,在消息生产时,查询数据库带上当前账号数据的版本号,在消费时,校验版本号是否一致,也可以保证消费端的幂等性.

**降低消息队列中消息的延迟**

监控消息的堆积: Kafka 提供了工具叫做“kafka-consumer-groups.sh 或者使用JMX

```
./bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group test-consumer-group
```

　　自定义监控消息,内容可以是生成监控消息的时间差,监控程序消费到监控消息后,进行时间比较,如果满足时间差大于某个阈值可以进行 监控报警处理.

减少消息的延迟需要在队列中以及消费端进行处理. 消费端可以增加消费者,但是Kafka约定了一个partition只可被一个消费者进行消费.

需要注意消费者空转把CPU打满的情况.

使用零拷贝技术,提升消息队列的性能. 使用了 Sendfile 之后，在内 核缓冲区的数据不会被拷贝到用户缓冲区，而是直接被拷贝到 Socket 缓冲区，节省了一次 拷贝的过程，提升了消息发送的性能,Java 里面的 java.nio.channels.FileChannel 类就提供了 transferTo 方法提供了 Sendfile的功能。

***

### 分布式服务篇

单体架构的痛点: 数据库连接数可能成为系统的瓶颈,不仅需要支撑外网的流量,以及消息队列或者部门内部的连接. 代码业务逻辑复杂后,打包编译麻烦.

服务拆分应该坚持什么原则?拆分粒度?拆分边界?拆分后可能遇见的问题?

　　单一职责,初期可以先粗粒度的拆分,理清服务之间的调用关系,服务接口的定义要具备可扩展性,不再是同一个进程间,调用不同的方法,而是跨进程调用.需要选择高效的服务调用框架.拆分后运维复杂一点

　　引入服务治理体系, 针对出问题的服务，采用熔断、降 级、限流、超时控制的方法，使得问题被限制在单一服务中，保护服务网络中的其它服务不 受影响

　　引入分布式追踪工具, 一条请求的调用链路上，涉及多个服务，那么一旦这个请求的 响应时间增长，或者是出现错误,很难知道具体是哪个服务,需要 ，以及更细致的服务端监控报表

**提升RPC框架性能**

一次rpc流程: 客户端将需要调用的类名,方法参数等信息,序列化后通过网络传输到服务端,服务端反序列化后,通过动态代理,调用相关的服务,获取结果再进行网络传输到客户端,客户端再对结果进行解析.

网络传输和序列化网络传输涉及到5中IO模型: 同步阻塞IO(一直同步等待直到有结果), 同步非阻塞IO(轮询查询结果) 多路IO复用(同时做多件事情,哪一个先有结果,则采用), 信号驱动IO(有结果了给出一个信号通知), 异步IO(出结果了,主动将结果推送处理) 最常用的就是多路IO复用模型, 在linux中select和epoll都采用这种模型.　　Nagle\`s算法为了减少不必要的网络传输,没有收到之前发送的数据包的 Ack 信息，那么这些小数据包就会在发送端暂存起来，直到小数据包累积到一个 MSS（Maximum SegmentSize)，或者收到一个 Ack 为止。 Nagle's算法会增加延迟,需要关闭, socket 上开启 **tcp\_nodelay** 就好了,不需要等到上一个发送包的 ACK 返回，直接发送新的数据包,自定义RPC框架时,最好开启tcp\_nodelay.

　　选择合适的序列化,常见的方案:

json序列化,另外的 Thrift 和 Protobuf 都是需要引入 IDL（Interface description language）的，也就是需要按照约定的语法写一个 IDL 文件，然后通过特定的编译器将它转换成各语言对应的代码，从而实现跨语言的特点。

对于性能要求不高，在传输数据占用带宽不大的场景下，可以使用 JSON 作为序列化协议

**分布式注册中心寻址**

优雅停机: 服务向注册中心请求将服务下线,再判断是否有流量进入,最后关闭服务.

若服务异常宕机,无法向注册中心请求下线服务,再次调用服务就会出现异常!

解决1: RPC服务开启一个端口,又注册中心定时主动探测服务,若端口不通则剔除服务.但是这样探测很多的RPC服务会有高昂的成本,探测的时间也比较长，这样当一个服务不可用时，可能会有一段时间的延迟，才会被注册中心探测到 ,修改为心跳模式. RPC服务向注册中心发送心跳包 注册 中心在接受到心跳包之后，会更新这个节点的最近续约时间。然后，注册中心会启动一个定 时器，定期检测当前时间和节点，最近续约时间的差值，如果达到一个阈值（比如说 90 秒），那么认为这个服务节点不可用。

　　给注册中心增加了保护的策略：如果摘除的节点占到了服务集群 节点数的 40%，就停止摘除服务节点，并且给服务的开发同学和，运维同学报警处理,避免将服务全部下线.

"通知风暴" 问题: 100个服务调用者,100个节点,当100个节点下线需要发送100\*100条消息通知,严重占用机器 的带宽资源,可以通过扩容注册中心解决

**分布式Trace**

采用 traceId + spanId 这两个数据维度来记录服务之间的调用关系spanId:1.1.2 表示上级spanId是1.1,调用次序是2.

　　先从线程上下文中获取当前的 traceId 和 spanId，然后，依据上面的逻辑生成本次 RPC 调用的 spanId，再将 spanId 和 traceId 序 列化后，装配到请求体中，发送给服务方 B ,服务方 B 获取请求后，从请求体中反序列化出 spanId 和 traceId，同时设置到线程上下文 中，以便给下次 RPC 调用使用。在服务 B 调用完成返回响应前，计算出服务 B 的执行时间 发送给消息队列

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/trace.png" alt=""><figcaption></figcaption></figure>

**负载均衡**

　　nginx在应用层做负载均衡,LVS在传输层工作,也称为四层负载.LVS 是在网络栈的四层做请求包的转发，请求包转发之 后，由客户端和后端服务直接建立连接，后续的响应包不会再经过 LVS 服务器，所以相比 Nginx，性能会更高，也能够承担更高的并发

负载均衡服务大体上可以分为两大类：一类是代理类的负载均衡服务；另一类 是客户端负载均衡服务

客户端负载均衡是通过注册中心拿到服务列表,再决定选择哪一个服务进行调用.

负载均衡策略:

　　静态策略,最广泛的是轮询(带有权重的轮询)的策略,不会参考后端服务的实际情况.Nginx 提供了 ip\_hash 和 url\_hash 算法； LVS 提供了按照请求的源地址，和目的地址做 hash 的策略； Dubbo 也提供了随机选取策略，以及一致性 hash 的策略。

　　动态策略,在负载均衡服务器上会收集对后端服务的调用信息，比如从负载均衡端到后端服务的活跃连 接数，或者是调用的响应时间，然后从中选择连接数最少的服务，或者响应时间最短的后端服务.Dubbo 提供的 LeastAcive 策略，就是优先选择活跃连接数最少的服务,Ribbon 提供了 WeightedResponseTimeRule 是使用响应 时间，给每个服务节点计算一个权重，然后依据这个权重，来给调用方分配服务节点 .

nginx检测节点是否可用,使用nginx\_upstream\_check\_module了 Nginx 定期地探测后端服务的一个指定的接口，然后根据返回的状态码，来判断服务是否可用.

启动和关闭线上 Web 服务时的标准姿势: 在服务刚刚启动时，可以初始化默认的 HTTP 状态码是 500,在完全初始化之后，再将 HTTP 状态码变更为 200,经过nginx探测后就会将服务标记为可用,服务关闭时,先将 HTTP 状态码变更为 500，等待 Nginx 探测将服务 标记为不可用后，前端的流量也就不会继续发往这个服务节点。在等待服务正在处理的请求 全部处理完毕之后，再对服务做重启，可以避免直接重启导致正在处理的请求失败的问题。

**API网关: 系统的门面**

可以对出入系统的流量做统一的管控,解决一些服务治理的问题,客户端的认证授权,黑名单的过滤 ,API 网关可以分为两类：一类叫做入口网关，一类叫做出口网关.

入口网关: 从中提供协议转换、安全策略、认证、限流、熔断等功能

出口网关: 可以对当前服务调用的第三方服务api做统一的校验,认证授权.

API 网关的实现重点在于性能和扩展性，你可以使用多路 I/O 复用模型和线程池并发处 理，来提升网关性能，使用责任链模式来提升网关的扩展性

API网关的实现: 多个过滤器串联,即责任链模式

为了提升网关对于请求的并行处理能力,采用线程池处理,为了避免单个服务故障导致的线程池被全部占用,或者阻塞,可以对单个服务采用单个线程池,或者对线程池的单个服务设置一定的配额,避免影响其他服务.

常见的网关: kong, zuul, Tyk(go)

**多机房部署**

多机房部署的含义是：在不同的 IDC 机房中，部署多套服务，这些服务共享同一份业务数 据，并且都可以承接来自用户的流量 .

问题: AB两个机房部署了不同的服务,B机房如何访问A机房的数据库. 直接访问A机房从库,或者在B机房建立从库,同步A机房数据,但是存在延迟,且机房之间的数据同步还和距离带宽有关.

北京同地双机房之间的专线延迟一般在 1ms\~3ms

国内异地双机房之间的专线延迟会在 50ms 之内

1. 多机房部署方案之同城双活

同城可避免机房宕机问题,且延迟也可以接受,无法做到城市级别的容灾.

　　方案: 数据库的主库部署在A机房,AB各部署一个从库,通过主从复制 的方式，从主库中同步数据，这样双机房的查询请求可以查询本机房的从库,A机房出现故障,通过主从切换将B从库设置为主库.缓存也可以部署在两个机房,数据在缓存中不存在,直接查询本机房库. RPC客户端只订阅同机房的服务组,避免出现跨机房的RPC调用.

2.  多机房部署方案之异地多活

    异地多活数据同步方案,同步主库的数据到从库,或者主库写入数据后放入消息队列,从库从消息队列中读取.尽量 保证用户在读取自己的数据时，读取数据主库所在的机房,需要对用户 做分片，让一个用户每次的读写都尽量在同一个机房中

**Service Mesh:屏蔽服务化系统的服务治理细节**

Service Mesh 主要处理服务之间的通信

RPC 客户端将数据包先发送给，与自身同主机部署的 Sidecar，在 Sidecar 中经过服务发现、负载均衡、服务路由、流量控制之后，再将数据发往指定服务节点的 Sidecar，在服务节点的 Sidecar 中，经过记录访问日志、记录分布式追踪日志、限流之 后，再将数据发送给 RPC 服务端

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/service-mesh.png" alt=""><figcaption></figcaption></figure>

业界提及最多的 Service Mesh 方案当属istio,或者SOFAMesh

Service Mesh 分为数据平面和控制平面。数据平面主要负责数据的传输；控制平面用来 控制服务治理策略的植入。出于性能的考虑，一般会把服务治理策略植入到数据平面中，控 制平面负责服务治理策略数据的下发 .

Sidecar 的植入方式目前主要有两种实现方式，一种是使用 iptables 实现流量的劫持；另 一种是通过轻量级客户端(直接调用)来实现流量转发 或者还在探索的方案Cilium .

***

### 维护篇

**监控**

在服务层面一般需要 监控四个指标，分别是延迟，通信量、错误和饱和度

数据库主从延迟数据、消息队列的堆积情况、缓存的命中率,GC时间,GC频率,慢SQL,Tomcat线程池任务堆积数等

延迟指的是请求的响应时间

通信量可以理解为吞吐量，也就是单位时间内，请求量的大小。比如，访问第三方服务 的请求量，访问消息队列的请求量

错误表示当前系统发生的错误数量

饱和度指的是服务或者资源到达上限的程度（也可以说是服务或者资源的利用率），比 如说 CPU 的使用率，内存使用率，磁盘使用率，缓存数据库的连接数

可以使用JMX或者埋点(从客户端着手,分布式trace中提到的面向切面)或者Agent(部署在数据源的服务器上采集相关信息),也可以监控日志.

**监控数据的处理和存储**

监控数据放入消息队列,一种是处理程序放入ES,用kibana做展示,另一个是流式处理的中间件 Spark、Storm,将数据解析存储,并存放在时间序列数据库中.

**应用性能管理(APM): 监控用户体验**

**压力测试: 搭建全链路压测平台**

流量的隔离: 区别压测流量和正式流量

风险控制: 避免压力测试对于正常访问用户的影响

正常包含: 流量构造和产生模块； 压测数据隔离模块； 系统健康度检查和压测流量干预模块

轻型的流量拷贝工 具 GoReplay 它可以劫持本机某一个端口的流量，将它们记录在文件中，传送到流量数据工厂中。在压测时，可以使用这个工具进行加速的流量回放，这样就可以实现对正式环境的压力测试了.

压测产生的数据写入影子库(和真实库相同)

**配置管理**

修改 dirty\_writeback\_centisecs 参数 的数值，就可以调整 Page Cache 中脏数据刷新到磁盘上的频率

修改 tcp\_max\_syn\_backlog 参数置的值，来调整未建立连接队列的长度

统一将配置文件放到一个配置目录,或者使用配置中心

**配置变更推送**

方法1: 应用程序向配置中心客户端注册一个监听器，配置中心的客户端，定 期地 查询所需要的配置是否有变化，如果有变化则通知触发监听器，让应 用程序得到变更通知 ,由于配置项很多,返回配置的话占用带宽,所以计算出一个md5,返回md5给客户端,客户端比较发现不同,则再次去配置中心拉取配置,这样可以大大的减少带宽.

方法2: 建立长连接,可以实时的将配置推送到客户端.

**配置中心高可用**

让配置中心“旁路化 即使配置中心宕机，或者配置中心依赖的存储宕机，我们仍然能够保证应用程序 是可以启动的.

配置中心客户端在获取到配置信息后，会同时把配置信息同步地写入到内存缓存，并且异步 地写入到文件缓存中。内存缓存的作用是降低客户端和配置中心的交互频率，提升配置获取 的性能；而文件的缓存的作用就是灾备，当应用程序重启时，一旦配置中心发生故障，那么 应用程序就会优先使用文件中的配置，这样虽然无法得到配置的变更消息（因为配置中心已 经宕机了），但是应用程序还是可以启动起来的，算是一种降级的方案

**降级熔断: 屏蔽非核心系统故障的影响**

服务治理中的熔断机 制指的是在发起服务调用的时候，如果返回错误或者超时的次数超过一定阈值，则后续的请求不再发向远程服务而是暂时返回错误.

维护一个状态机: 有关闭（调用远程服务）、半打开 （尝试调用远程服务）和打开（返回错误）三种状态

当调用失败的次数累积到一定的阈值时，熔断状态从关闭态切换到打开态。一般在实现 时，如果调用成功一次，就会重置调用失败次数

<figure><img src="https://cdn.jsdelivr.net/gh/yunCrush/yc-image/image/%E7%86%94%E6%96%AD%E5%99%A8.png" alt=""><figcaption></figcaption></figure>

熔断器状态切换演示代码

```

if (breaker.isOpen()) {
            // 断路器打开则直接返回空值
            return null; 
        }
        K value = null;
        Jedis jedis = null;
        try {
            jedis = connPool.getResource();
            value = callback.call(jedis);
            if(breaker.isHalfOpen()) { // 如果是半打开状态
                if(successCount.incrementAndGet() >= SUCCESS_THRESHOLD) {// 成功次数超
                    failCount.set(0); // 清空失败数
                    breaker.setClose(); // 设置为关闭态
                }
            }
            return value;
        } catch (JedisException je) {
            if(breaker.isClose()){ // 如果是关闭态
                if(failCount.incrementAndGet() >= FAILS_THRESHOLD){ // 失败次数超过阈值
                    breaker.setOpen(); // 设置为打开态
                }
            } else if(breaker.isHalfOpen()) { // 如果是半打开态
                breaker.setOpen(); // 直接设置为打开态
            }
            throw je;
        } finally {
            if (jedis != null) {
                jedis.close();
            }
        }
```

**降级**

预先设置一些开关来控制服务的返回值,本质就是提前设置一个默认的返回值.

示例代码:

```

    boolean switcherValue = getFromConfigCenter("degrade.comment"); // 从配置中心获取
        if (!switcherValue) {
            List<Comment> comments = getCommentList(); // 开关关闭则获取评论数据
        } else {
            List<Comment> comments = new ArrayList(); // 开关打开，则直接返回空评论数据
        }
```

**流量控制**

令牌桶算法(应对突发流量时,有充分的令牌可以使用),漏斗算法(应对突发流量时,是将流量缓存再漏斗中,并不推荐),滑动窗口算法(接收方返回ACK时会带上当前的空闲的缓冲区大小,发送方就可以调整发送速率)都可以限流.
